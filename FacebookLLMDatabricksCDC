# Tom Lorenc Facebook LLM Databricks Schoolclosure Databricks Pipeline

## Overview

The Schoolclosure Databricks Pipeline is designed to automate the process of tracking unplanned school closures in the United States. It leverages the power of Databricks, Machine Learning, Generative AI, and the Facebook Graph API to process and analyze large volumes of data related to school closures.

## Pipeline

The automated pipeline is initiated via a daily Databricks Workflow which triggers the FacebookOrchestrator notebook. This notebook calls other downstream notebooks in succession to execute the full pipeline. Those notebooks include:

- FacebookIngest: Retreiving all new posts from schools whose Facebook account is stored in the database. This notebook leverages the Facebook Graph API via a REST API call.
- FacebookFilter: Filtering all newly retrieved posts via a regex filter, quickly eliminating completely irrelevant posts.
- FacebookAnalyze: Uses a fine-tuned BERT-based model in pytorch to classify each remaining post as related or unrelated to unplanned school closure.
    - see model_card.md for more information about the model.
- FacebookExtract: For posts tagged as related to unplanned school closure, passes each post to a LLM via the Azure OpenAI API (gpt-3.5-turbo), requesting standardized information via crafted prompts.
- FacebookReports: Final tagged posts are combined with metadata about the school/district from their posting account, then all relevant data is posted to the final table for daily report.

### Pipeline Flow

![Pipeline Flow Diagram](./img/architecture_flow.png "Pipeline Overview")

### Architecture Diagram

![Architecture Diagram](./img/architecture.png "Architecture Diagram")

## Dashboard

The SchoolClosure dashboard is generated in Databricks using the SQL queries found in the `./sql/` directory. This dashboard runs every morning, following the pipeline run, and sends an automated email to subscribers.

## Data Storage

All data is stored in Databricks in the `schoolclosure_adl` schema. These tables can be recreated using the queries found in the `./sql/create_tables.sql` file.

The data found in the `schools` and `districts` tables was pulled from the NCED school finder website. Facebook accounts were detected in an automated manner using the `./utilities/SchoolClosure_FacebookAccountFinder.py` notebook.

## Necessary keys

In order to operate this pipeline, you need to have several authorization keys. Those are as follows:

* Facebook Graph API: To retrieve posts from public facebook pages, you need to have an API key with the "Page Public Content Access" permission enabled. This requires permission from Meta via Meta's Developer portal.
* Azure OpenAI: The pipeline is designed to interact with Azure OpenAI via a specific deployment, with keys stored as Databricks secrets. You can easily convert to a different LLM using the various connectors found in the [langchain python library](https://www.langchain.com/). 

## Getting Started

To get started with the Schoolclosure Databricks Pipeline, follow these steps:

1. Clone the repository in your databricks workspace.
2. Obtain necessary keys and populate in databricks secrets.
3. Create all tables in Databricks database.
4. Populate `schools` and `districts` tables using data from NCES.
from typing import List, Dict, Tuple
import pandas as pd
from sentence_transformers import CrossEncoder
from langchain import OpenAI, PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import numpy as np

class SchoolClosureRAG:
    def __init__(self, openai_api_key: str):
        # Initialize models
        self.embeddings = HuggingFaceEmbeddings()
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        self.llm = OpenAI(api_key=openai_api_key)
        
        # Initialize vector store
        self.vector_store = None
        
    def simulate_school_closure_data(self, n_samples: int = 100) -> List[Dict]:
        """Simulate school closure data for testing"""
        reasons = ['Weather', 'Infrastructure', 'Health', 'Security', 'Staffing']
        districts = [f'District_{i}' for i in range(1, 6)]
        
        data = []
        for _ in range(n_samples):
            data.append({
                'school_name': f'School_{np.random.randint(1, 21)}',
                'district': np.random.choice(districts),
                'closure_reason': np.random.choice(reasons),
                'closure_date': pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(0, 30)),
                'reopening_date': pd.Timestamp.now() + pd.Timedelta(days=np.random.randint(1, 7)),
                'description': f'School closure due to {np.random.choice(reasons).lower()} issues.'
            })
        return data
    
    def index_documents(self, documents: List[Dict]):
        """Index documents in vector store"""
        texts = [f"{doc['school_name']} - {doc['description']}" for doc in documents]
        self.vector_store = FAISS.from_texts(texts, self.embeddings)
        return len(texts)
    
    def retrieve_and_rerank(self, query: str, k: int = 5) -> List[Tuple[str, float]]:
        """Retrieve relevant documents and rerank them"""
        # Initial retrieval
        docs = self.vector_store.similarity_search_with_score(query, k=k*2)
        
        # Prepare for reranking
        texts = [doc[0].page_content for doc in docs]
        pairs = [[query, text] for text in texts]
        
        # Rerank
        scores = self.reranker.predict(pairs)
        ranked_results = list(zip(texts, scores))
        ranked_results.sort(key=lambda x: x[1], reverse=True)
        
        return ranked_results[:k]
    
    def generate_response(self, query: str, context: List[Tuple[str, float]]) -> str:
        """Generate response using LLM with retrieved context"""
        prompt_template = """
        Based on the following school closure information:
        {context}
        
        Please provide a detailed analysis addressing this question: {query}
        
        Focus on:
        1. Patterns in closure reasons
        2. Duration of closures
        3. Geographic distribution
        4. Potential impact on education
        
        Response:
        """
        
        context_str = "\n".join([f"- {text} (relevance: {score:.2f})" for text, score in context])
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "query"]
        )
        
        return self.llm(prompt.format(context=context_str, query=query))

def main():
    # Initialize system
    rag_system = SchoolClosureRAG("your-openai-api-key")
    
    # Simulate and index data
    data = rag_system.simulate_school_closure_data(100)
    num_indexed = rag_system.index_documents(data)
    print(f"Indexed {num_indexed} documents")
    
    # Example query
    query = "What are the most common reasons for school closures in the past week?"
    
    # Retrieve and rerank
    ranked_results = rag_system.retrieve_and_rerank(query)
    print("\nTop ranked relevant documents:")
    for text, score in ranked_results:
        print(f"\nScore: {score:.4f}")
        print(f"Content: {text}")
    
    # Generate response
    response = rag_system.generate_response(query, ranked_results)
    print(f"\nGenerated Response:\n{response}")

if __name__ == "__main__":
    main()
