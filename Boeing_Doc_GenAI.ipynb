{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58151d-21a9-4526-952e-0f396876d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Generative AI system for document-based question answering for Boeing.\n",
    "\n",
    "A use-case of AI for intelligent information retrieval and natural language processing, capable of fetching information from structured document data and generating human-like responses to queries based on the content of the Boeing documents.\n",
    "\n",
    "Document Vector Retrieval (get_document_vector): This function retrieves the vector representation of a document from a database (like Pinecone) using the document's ID. The vector is a numerical representation that captures various aspects of the document (such as summary, keywords, title, and category).\n",
    "\n",
    "Document Indexing: The code iterates through a list of documents, each represented by a dictionary with an 'id' and a 'vector'. These vectors are then indexed or updated in the database using the index.upsert method.\n",
    "\n",
    "Relevance Scoring and Selection (select_most_relevant_document): This function calculates the relevance of each document to a given query. It computes cosine similarity scores between the query's vector and each document's vector components (summary, keywords, title, category), and combines these scores to determine the most relevant document.\n",
    "\n",
    "Query Processing (process_query): This central function processes the user's query by:\n",
    "Extracting important keywords and summarizing the query.\n",
    "\n",
    "Vectorizing the summary and keywords using a model (e.g., an NLP model).\n",
    "\n",
    "Querying the indexed documents to find the top relevant documents.\n",
    "\n",
    "Applying additional logic (like considering document views) to select the most relevant document.\n",
    "\n",
    "Response Generation: Once the most relevant document is selected, its content is retrieved, and a response is generated using OpenAI's GPT-3 model. The model is prompted with the content of the relevant document and the user's query to generate an answer.\n",
    "\n",
    "Error Handling: The response generation is wrapped in a try-except block to handle any potential errors during the response generation process.\n",
    "\n",
    "Main Function (main): This function is the entry point of the program. It sets up necessary data like views count for documents and the OpenAI API key, takes a sample question (e.g., about Boeing 787's engine maintenance safety protocols), and then processes this query to generate a response.\n",
    "\n",
    "Execution Check: The if __name__ == \"__main__\": condition checks if the script is run as the main program and not imported as a module. If it's the main program, it executes the main() function.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "def get_document_vector(doc_id):\n",
    "    # Query Pinecone for the vector of the given document ID\n",
    "    response = index.query(ids=[doc_id])\n",
    "    if response['results'][0]['matches']:\n",
    "        # Extract the vector from the response\n",
    "        doc_vector = response['results'][0]['matches'][0]['vector']\n",
    "        return doc_vector\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Assuming `documents` is a list of dictionaries, where each dictionary\n",
    "# represents a document with its 'id' and 'vector'\n",
    "for document in documents:\n",
    "    # Here, 'vector' is the concatenated vector of summary, keywords, title, and category\n",
    "    index.upsert(ids=[document['id']], vectors=[document['vector']])\n",
    "\n",
    "\n",
    "def select_most_relevant_document(doc_ids, query_vector, weights):\n",
    "    highest_score = 0\n",
    "    most_relevant_doc = None\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        # Retrieve the document's vector (summary, keywords, title, category)\n",
    "        doc_vector = get_document_vector(doc_id)  # Implement this function\n",
    "\n",
    "        # Compute similarity scores\n",
    "        summary_similarity = compute_cosine_similarity(query_vector['summary'], doc_vector['summary'])\n",
    "        keyword_similarity = compute_cosine_similarity(query_vector['keywords'], doc_vector['keywords'])\n",
    "        title_similarity = compute_cosine_similarity(query_vector['title'], doc_vector['title'])\n",
    "        category_similarity = compute_cosine_similarity(query_vector['category'], doc_vector['category'])\n",
    "\n",
    "        # Combine scores\n",
    "        combined_score = combine_scores(summary_similarity, keyword_similarity, title_similarity, category_similarity, weights)\n",
    "\n",
    "        # Check for the highest score\n",
    "        if combined_score > highest_score:\n",
    "            highest_score = combined_score\n",
    "            most_relevant_doc = doc_id\n",
    "\n",
    "    return most_relevant_doc\n",
    "\n",
    "\n",
    "def process_query(query):\n",
    "    # Extract important keywords and summarize the query\n",
    "    query_keywords = extract_important_keywords(query)\n",
    "    query_summary = summarize_text(query)\n",
    "\n",
    "    # Vectorize summary and keywords\n",
    "    query_vector = model.encode([query_summary] + [' '.join(query_keywords)])\n",
    "\n",
    "    # Fetch relevant document IDs from Pinecone\n",
    "    results = index.query(query_vector, top_k=3)\n",
    "    relevant_documents = [result['id'] for result in results['matches']]\n",
    "\n",
    "    # Additional logic to choose the most relevant document\n",
    "    relevant_document_id = select_most_relevant_document(relevant_documents, query_vector)\n",
    "\n",
    "    # Generate response using OpenAI and Langchain\n",
    "    response = prompt(f\"Using the information from document {relevant_document_id}, answer: {query}\")\n",
    "    return response\n",
    "\n",
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "\n",
    "def combine_scores(summary_score, keyword_score, summary_weight=0.5):\n",
    "    return summary_weight * summary_score + (1 - summary_weight) * keyword_score\n",
    "\n",
    "\n",
    "def select_most_relevant_document(doc_ids, query_vector, summary_weight=0.5):\n",
    "    highest_score = 0\n",
    "    most_relevant_doc = None\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        # Retrieve the document's vector (summary + keywords)\n",
    "        doc_vector = get_document_vector(doc_id)  # Implement this function\n",
    "\n",
    "        # Compute similarity scores\n",
    "        summary_similarity = compute_cosine_similarity(query_vector[:len(query_vector)//2], doc_vector[:len(doc_vector)//2])\n",
    "        keyword_similarity = compute_cosine_similarity(query_vector[len(query_vector)//2:], doc_vector[len(doc_vector)//2:])\n",
    "\n",
    "        # Combine scores\n",
    "        combined_score = combine_scores(summary_similarity, keyword_similarity, summary_weight)\n",
    "\n",
    "        # Check if this is the highest score so far\n",
    "        if combined_score > highest_score:\n",
    "            highest_score = combined_score\n",
    "            most_relevant_doc = doc_id\n",
    "\n",
    "    return most_relevant_doc\n",
    "\n",
    "\n",
    "\n",
    "def combine_scores(summary_score, keyword_score, views_score, summary_weight=0.3, views_weight=0.4):\n",
    "    \"\"\"\n",
    "    Combine the summary, keyword, and views scores into a single score.\n",
    "    Adjust the weights of each component as needed.\n",
    "    \"\"\"\n",
    "    return summary_weight * summary_score + (1 - summary_weight - views_weight) * keyword_score + views_weight * views_score\n",
    "\n",
    "\n",
    "def select_most_relevant_document(doc_ids, query_vector, views_dict, summary_weight=0.3):\n",
    "    \"\"\"\n",
    "    Select the most relevant document based on cosine similarity and number of views.\n",
    "    `views_dict` is a dictionary mapping document IDs to their view counts.\n",
    "    \"\"\"\n",
    "    highest_score = 0\n",
    "    most_relevant_doc = None\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        # Retrieve the document's vector (summary + keywords)\n",
    "        doc_vector = get_document_vector(doc_id)  # Implement this function\n",
    "\n",
    "        # Compute similarity scores\n",
    "        summary_similarity = compute_cosine_similarity(query_vector[:len(query_vector)//2], doc_vector[:len(doc_vector)//2])\n",
    "        keyword_similarity = compute_cosine_similarity(query_vector[len(query_vector)//2:], doc_vector[len(doc_vector)//2:])\n",
    "\n",
    "        # Get the views score (normalized)\n",
    "        views_score = normalize_views_score(views_dict.get(doc_id, 0))\n",
    "\n",
    "        # Combine scores\n",
    "        combined_score = combine_scores(summary_similarity, keyword_similarity, views_score, summary_weight)\n",
    "\n",
    "        # Check if this is the highest score so far\n",
    "        if combined_score > highest_score:\n",
    "            highest_score = combined_score\n",
    "            most_relevant_doc = doc_id\n",
    "\n",
    "    return most_relevant_doc\n",
    "\n",
    "def normalize_views_score(views_count):\n",
    "    \"\"\"\n",
    "    Normalize the views count to a score between 0 and 1.\n",
    "    This function can be adjusted based on how you want to scale the views count.\n",
    "    \"\"\"\n",
    "    return min(1, views_count / 1000) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_query(query, openai_api_key):\n",
    "    # Extract important keywords and summarize the query\n",
    "    query_keywords = extract_important_keywords(query)\n",
    "    query_summary = summarize_text(query)\n",
    "\n",
    "    # Vectorize summary and keywords\n",
    "    query_vector = model.encode([query_summary] + [' '.join(query_keywords)])\n",
    "\n",
    "    # Fetch relevant document IDs from Pinecone\n",
    "    results = index.query(query_vector, top_k=3)\n",
    "    relevant_documents = [result['id'] for result in results['matches']]\n",
    "\n",
    "    # Additional logic to choose the most relevant document\n",
    "    relevant_document_id = select_most_relevant_document(relevant_documents, query_vector)\n",
    "\n",
    "    # Retrieve the content of the most relevant document\n",
    "    document_content = retrieve_document_content(relevant_document_id)  # Implement this function\n",
    "\n",
    "    # Generate response using OpenAI GPT-3\n",
    "    openai.api_key = openai_api_key\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",  # Or the latest available engine\n",
    "            prompt=f\"Using the information from the following Boeing engineering document: {document_content}\\n\\nAnswer the query: {query}\",\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {e}\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Example Boeing doc views data and API key\n",
    "    views_dict = {\"doc1\": 100, \"doc2\": 50, ...}\n",
    "    openai_api_key = \"your-api-key-here\"\n",
    "\n",
    "    # Sample Boeing question\n",
    "    question = \"What are the safety protocols for Boeing 787's engine maintenance?\"\n",
    "\n",
    "    # Process the query\n",
    "    response = process_query(question, openai_api_key)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
